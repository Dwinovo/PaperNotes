

> In this section, we conducted several experiments to test the performance of the proposed VAE-Stega model.
> 在本节中，我们进行了多项实验来测试所提出的 VAE-Stega 模型的性能。
📌 **分析：**
* 开宗明义，表明本节的主题。

---

> Firstly, we introduce the dataset used in our experiment, as well as the model settings and training details.
> 首先，我们介绍了实验中使用的数据集，以及模型设置和训练细节。
📌 **分析：**
* 预告了本节内容的第一个部分：数据准备和模型训练细节，这是任何可复现实验的基础。

---

> Then, we mainly analyze the imperceptibility of the steganographic sentences generated by the proposed model and some other previous related works.
> 然后，我们主要分析了所提出的模型以及其他一些先前相关工作生成的隐写语句的**不可察觉性（imperceptibility）**。
📌 **分析：**
* 预告了本节内容的第二个主要部分：重点评估 VAE-Stega 和其他基线模型的“不可察觉性”，这呼应了论文核心问题（Psic Effect）和解决方案（平衡感知与统计不可察觉性）。

---

> A. Data Preparing and Model Training
> A. **数据准备与模型训练**
📌 **分析：**
* 本小节将详细阐述实验中使用的数据集、预处理步骤以及模型的具体配置和训练过程。

---

> In this work, we used the two large-scale public text datasets to train our model, which were Twitter [53] and IMDB movie reviews [39].
> 在这项工作中，我们使用了两个大规模的公共文本数据集来训练我们的模型，它们是 Twitter [cite: 53] 和 IMDB 电影评论 [cite: 39]。
📌 **分析：**
* **数据集选择**：选择了两个广泛使用的、具有代表性的大规模公共文本数据集，这有助于提高实验结果的可信度和可比较性。

---

> After pre-processing, which included converting all words into lowercase, deleting special symbols, emoticons, web links, and filtering low-frequency words, Twitter [53] dataset contained a total of 2,639,083 normal sentences with a dictionary size of 44,856, IMDB dataset [39] contained a total of 1,282,804 normal sentences with a dictionary size of 48,042.
> 经过预处理，包括将所有词转换为小写、删除特殊符号、表情符号、网页链接和过滤低频词后，Twitter 数据集 [cite: 53] 总共包含 2,639,083 条正常语句，词典大小为 44,856；IMDB 数据集 [cite: 39] 总共包含 1,282,804 条正常语句，词典大小为 48,042。
📌 **分析：**
* **预处理步骤**：这些是文本数据处理的常见和标准操作，旨在清理数据、减少噪声、统一格式并控制词汇量，以便模型更有效地学习。
* **数据集规模和词典大小**：提供了具体的数据量信息，表明数据集足够大，能够支持深度学习模型的训练，并反映了处理后词汇的丰富程度。

---

> Almost all the parameters in VAE-Stega model could be obtained through training, but there were still some hyper-parameters need to be determined.
> VAE-Stega 模型中几乎所有参数都可以通过训练获得，但仍有一些**超参数（hyper-parameters）**需要确定。
📌 **分析：**
* **“超参数”**：在模型训练开始前需要手动设定的参数，例如学习率、层数、单元数、维度等。它们无法通过梯度下降等训练算法自动优化，通常需要通过经验、网格搜索或随机搜索等方法来确定。

---

> Through multiple comparison experiments, these hyper-parameters were final set to be as follows.
> 通过多次比较实验，这些超参数最终设置如下。
📌 **分析：**
* 表明超参数的设定是经过了实验验证和优化的，而非随意选择。

---

> For decoder, we used the same settings as RNN-Stega [13] to generated sentences, namely, we set the number of LSTM hidden layers to be 3, with each layer containing 800 LSTM units.
> 对于解码器，我们使用了与 RNN-Stega [cite: 13] 相同的设置来生成句子，即：我们将 LSTM 隐藏层的数量设置为 3 层，每层包含 800 个 LSTM 单元。
📌 **分析：**
* **解码器设置**：与基线模型 RNN-Stega 保持一致的解码器配置，这有助于公平比较，确保 VAE-Stega 的性能提升主要归因于 VAE 架构本身，而不是解码器部分的改进。

---

> We used the B E RTB AS E [48] as the encoder in VAE-Stega (BERT-LSTM), which contained 12 Transformer blocks with a hidden size of 768, and the number of self-attention heads was 12.
> 我们在 VAE-Stega (BERT-LSTM) 中使用了 **$BERT_{BASE}$** [cite: 48] 作为编码器，它包含 12 个 Transformer 块，隐藏层大小为 768，自注意力头数量为 12。
📌 **分析：**
* **BERT 编码器设置**：$BERT_{BASE}$ 是 BERT 模型的一种标准配置，其参数量较大，具有强大的特征提取能力。提供了具体的结构参数，如层数、维度和注意力头数。

---

> We set the dimension of latent space to be 13, the number of Highway layers was 2 with a hidden dimension of 1600. For encoder in VAE-Stega (LSTM-LSTM) model, we set it the same as the decoder.
> 我们将潜在空间的维度设置为 13，Highway 层的数量为 2，隐藏层维度为 1600。对于 VAE-Stega (LSTM-LSTM) 模型中的编码器，我们将其设置与解码器相同。
📌 **分析：**
* **潜在空间维度**：13 是一个相对较小的维度，但足够捕捉文本的复杂统计特征。
* **Highway network 设置**：提供了 Highway 层的具体配置，用于从 BERT/LSTM 输出中学习潜在分布的均值和方差。
* **LSTM-LSTM 编码器设置**：与解码器保持一致，即3层LSTM，每层800单元。

---

> Each word in the dictionary was mapped to an 353-dimensional vector and we used tanh as nonlinear activation function σ in Equation(21).
> 词典中的每个词都被映射到 353 维向量，并且我们在公式 (21) 中使用 tanh 作为非线性激活函数 $\sigma$。
📌 **分析：**
* **词嵌入维度**：353 维，这是词向量的维度，代表了每个词的语义信息。
* **激活函数**：澄清了公式 (21) 中 $\sigma$ 实际使用的是 tanh 函数，尽管在 LSTM 单元中 $\sigma$ 通常指 sigmoid 函数，但这里可能是指内部的激活函数。

---

> All the parameters of the neural network need to be obtained through training.
> 神经网络的所有参数都需要通过训练获得。
📌 **分析：**
* 这是深度学习模型的普遍特征，模型通过数据学习来优化其内部参数（权重和偏置）。

---

> In the training process, we updated network parameters using backpropagation algorithm.
> 在训练过程中，我们使用**反向传播算法（backpropagation algorithm）**更新网络参数。
📌 **分析：**
* **“反向传播算法”**：深度学习中最常用的训练算法，通过计算损失函数相对于模型参数的梯度，并沿着梯度下降的方向更新参数。

---

> By minimizing the loss function, we hope that the proposed model can learn both the statistical language model of normal sentences and their overall distribution pattern.
> 通过最小化损失函数，我们希望所提出的模型能够学习正常语句的统计语言模型及其整体分布模式。
📌 **分析：**
* 再次强调了 VAE-Stega 模型的双重优化目标：既要保证局部语言模型（高质量），又要保证整体统计分布（安全性）。这正是 VAE 损失函数（重构损失 + KL散度）所体现的。

---

> In the process of model training, we adopted the **KL cost annealing strategy** [47], so that the model can better take into account the language model and overall statistical distribution constraints, which makes the steganographic sentences generated by the decoder can have both high perceptual-imperceptibility and statistical-imperceptibility.
> 在模型训练过程中，我们采用了 **KL 成本退火策略（KL cost annealing strategy）** [cite: 47]，以便模型能更好地兼顾语言模型和整体统计分布约束，这使得解码器生成的隐写语句能够同时具有高感知不可察觉性和高统计不可察觉性。
📌 **分析：**
* **“KL 成本退火策略”** [cite: 47]：这是 VAE 训练中的一个重要技巧。在训练初期，KL 散度项（正则化项）的权重通常设置为0或很小，让模型优先学习数据的重构（即生成高质量文本）。随着训练的进行，KL 散度项的权重逐渐增加，强制潜在空间分布更接近先验分布。这样做是为了防止模型在早期训练时被过强的正则化项“惩罚”而难以学习生成有效数据。它有助于模型更好地平衡重构质量和潜在空间分布的匹配，从而有效解决 Psic Effect。

---

> Figure 7 shows the values of Loss varying with the number of iterations during training process of VAE-Stega (LSTM-LSTM) model on IMDB dataset.
> 图7 显示了 VAE-Stega (LSTM-LSTM) 模型在 IMDB 数据集上训练过程中，损失值随迭代次数变化的曲线。
📌 **分析：**
* 引导读者查看图7，该图将直观展示 KL 成本退火策略的效果以及模型如何平衡两种损失。
![[Pasted image 20250521153306.png]]
---

> The red line represents reconstruction loss which calculated as perplexity of generated sentences and the blue line represents KL divergence of $q_{\phi}(z|x)$ and $p_{\theta} (z)$, that is $D_{KL} (q_{\phi}(z|x )|| p_{\theta} (z))$.
> 红线表示**重构损失（reconstruction loss）**，其值以生成语句的困惑度计算；蓝线表示 $q_{\phi}(z|x)$ 和 $p_{\theta} (z)$ 的 **KL 散度（KL divergence）**，即 $D_{KL} (q_{\phi}(z|x )|| p_{\theta} (z))$。
📌 **分析：**
* 解释了图7中两条曲线的含义，并再次强调了困惑度作为重构损失（感知不可察觉性）的衡量，以及 KL 散度作为正则化损失（统计不可察觉性）的衡量。

---

> From Figure 7, we can see clearly how the proposed VAE-Stega model tries to coordinate the perceptual-imperceptibility and statistical-imperceptibility of the generated steganographic sentences under the influence of the Psic Effect.
> 从图7中，我们可以清楚地看到所提出的 VAE-Stega 模型如何在 Psic 效应的影响下，尝试协调生成的隐写语句的感知不可察觉性和统计不可察觉性。
📌 **分析：**
* 指出图7是理解 VAE-Stega 解决 Psic Effect 机制的关键。

---

> When the model training started, the proposed VAE-Stega model first quickly optimized the reconstruction error, that is, optimized the language model to ensure perceptual-imperceptibility.
> 当模型训练开始时，所提出的 VAE-Stega 模型首先快速优化了重构误差，即优化了语言模型以确保感知不可察觉性。
📌 **分析：**
* **训练初期**：重构损失（困惑度）迅速下降，说明模型优先学习生成高质量的、流畅的文本。这正是 KL 成本退火策略的效果。

---

> This part of the training process is similar to the previous automatic steganographic text generation model [13], [16], [17].
> 训练过程的这一部分与以前的自动隐写文本生成模型 [cite: 13, 16, 17] 类似。
📌 **分析：**
* 强调了在保证文本质量方面，VAE-Stega 与现有模型有着共同的起点。

---

> However, as we analyzed in Section III, a good language model can only guarantee the perceptual-imperceptibility of generated sentences.
> 然而，正如我们在第三节中分析的，一个好的语言模型只能保证生成句子的感知不可察觉性。
📌 **分析：**
* 再次重申了“质量不等于安全”的观点。

---

> But the overall distribution is still far from that of normal sentences, which can be seen from Figure 7, that is, when the model was iterated to nearly 2000 steps, the reconstruction error had been optimized to be small, but the value of $D_{KL}(q_{\phi}(z|x)|| p_{\theta}(z))$ was still very large.
> 但整体分布仍然与正常语句相去甚远，这可以从图7中看出，即当模型迭代到接近2000步时，重构误差已经优化得很小，但 $D_{KL}(q_{\phi}(z|x)|| p_{\theta}(z))$ 的值仍然非常大。
📌 **分析：**
* **关键观察**：即使重建误差（文本质量）已经很好，潜在空间分布（统计不可察觉性）仍然很差。这印证了 Psic Effect 的存在，并说明了仅优化语言模型是不够的。

---

> As the training process continues, the model further optimized the overall statistical distribution differences under the guidance of the loss function as shown in Formula (10).
> 随着训练过程的继续，模型在公式 (10) 所示的损失函数的指导下，进一步优化了整体统计分布差异。
📌 **分析：**
* **训练中期**：在 KL 成本退火策略的作用下，KL 散度项的权重增加，模型开始着重优化潜在空间分布，使其更接近标准正态分布，从而减小整体统计分布差异。

---

> When the KL divergence error decreases gradually, we can see that the reconstruction error increases a little.
> 当 KL 散度误差逐渐减小时，我们可以看到重构误差略有增加。
📌 **分析：**
* **平衡与权衡**：这是 Psic Effect 的体现。为了让整体分布更“像”正常文本，模型不得不牺牲一点点的重建质量，即让生成的文本不那么“完美”。

---

> This is also consistent with our previous analysis in section III, that is, we do not need to deliberately optimize the language model to the extreme to ensure the quality of each generated steganographic sentence.
> 这也与我们在第三节的分析一致，即我们不需要刻意将语言模型优化到极致来确保每个生成的隐写语句的质量。
📌 **分析：**
* 重申了“极致优化”的弊端：过于完美反而不自然。

---

> Because sentences on social network are written by different people from different backgrounds in a casual environment, they do not necessarily follow the optimal language model.
> 因为社交网络上的句子是由不同背景的人在休闲环境下以不同方式书写的，它们不一定遵循最优语言模型。
📌 **分析：**
* 再次强调了真实文本的**多样性**和**非完美性**，为牺牲一点感知质量来换取统计隐蔽性提供了合理性。

---

> If we desperately optimize the quality of each generated sentence, it will cause inconsistencies with the statistical characteristics of the normal sample.
> 如果我们拼命优化每个生成语句的质量，它将导致与正常样本的统计特征不一致。
📌 **分析：**
* 指出了“过度优化”的后果：导致文本在统计上显得过于“一致”或“模式化”，从而与真实文本的“随机性”和“多样性”不符，容易被隐写分析发现。

---

> In order to ensure consistency with the statistical characteristics of normal sentences, we even need to sacrifice sentence quality a little to make it not “so perfect".
> 为了确保与正常语句的统计特征一致，我们甚至需要稍微牺牲语句质量，使其不“那么完美”。
📌 **分析：**
* 明确了在解决 Psic Effect 时的策略：主动地引入一些“不完美”，使其更接近真实文本的统计特性，从而提高整体隐蔽性。

---

> With the further training of the proposed model, in the end, the two error terms will be optimized to a stable and optimal state, which will also make the two aspects of the imperceptibility of generated steganographic sentences to achieve an optimal balance.
> 随着所提出模型的进一步训练，最终，两个误差项将被优化到一个稳定和最优的状态，这也将使生成的隐写语句的不可察觉性的两个方面达到一个最优的平衡。
📌 **分析：**
* 总结了 VAE-Stega 训练的最终目标和效果：通过 KL 成本退火和损失函数的设计，模型能够找到感知不可察觉性与统计不可察觉性之间的最佳平衡点。

---

> B. Imperceptibility Analysis
> B. **不可察觉性分析**
📌 **分析：**
* 本小节将详细介绍如何量化评估 VAE-Stega 生成的隐写文本的“不可察觉性”，包括具体的指标和对比实验。

---

> To measure the imperceptibility of the steganographic sentences generated by the proposed VAE-Stega model, according to the previous analysis, on the one hand, we need to test the language model of the generated steganographic sentences, which is measured by per plexi t y (ppl); on the other hand, we need to test the differences between the overall distribution of generated steganographic sentences and that of normal sentences, which is measured by Kullback-Leibler divergence (KLD) and Jensen-Shannon divergence (JSD).
> 为了衡量所提出的 VAE-Stega 模型生成的隐写语句的不可察觉性，根据先前的分析，一方面，我们需要测试生成隐写语句的语言模型，这通过**困惑度（perplexity, ppl）**来衡量；另一方面，我们需要测试生成隐写语句的整体分布与正常语句的整体分布之间的差异，这通过**库尔巴克-莱布勒散度（Kullback-Leibler divergence, KLD）**和**詹森-香农散度（Jensen-Shannon divergence, JSD）**来衡量。
📌 **分析：**
* **评估指标**：明确了三种主要的量化评估指标：
    * **困惑度 (PPL)**：衡量**感知不可察觉性**（文本质量）。
    * **KL 散度 (KLD)** 和 **JS 散度 (JSD)**：衡量**统计不可察觉性**（整体分布相似性）。

---

> In the field of natural language processing, per plexi t y is a standard measure for language model testing [40].
> 在自然语言处理领域，困惑度是语言模型测试的标准度量 [cite: 40]。
📌 **分析：**
* 重申了困惑度作为标准指标的地位。

---

> It is defined as the average per-word log-probability on the test texts: per plexi t y = 2− 1 n log p(x) = 2− 1 n log p(x1,x2,x3,...,xn ) = 2− 1 n $\sum_{j=1}^n log p(x j |x1,x2,...,x j−1)$, (27)
> 它被定义为测试文本上的平均每词对数概率：
> $$\text{perplexity} = 2^{-\frac{1}{n}\log p(x)} = 2^{-\frac{1}{n}\log p(x_1,x_2,x_3,...,x_n)} = 2^{-\frac{1}{n} \sum_{j=1}^n \log p(x_j |x_1,x_2,...,x_{j−1})} \quad \text{(27)}$$
📌 **分析：**
* 提供了困惑度的数学定义，即句子的对数概率的负平均值，然后取以2为底的指数。
* **$\log p(x)$**：整个句子的对数概率。
* **$\sum_{j=1}^n \log p(x_j |x_1,x_2,...,x_{j−1})$**：根据链式法则分解的句子对数概率。

---

> where x = {x1, x2, x3, . . . , xn} is the generated sentence and n is the length of it.
> 其中 $x = \{x_1, x_2, x_3, . . . , x_n\}$ 是生成的句子，$n$ 是其长度。
📌 **分析：**
* 对公式 (27) 中符号的解释。

---

> Usually, the smaller the value of per plexi t y, the better the language model of the sentences, which indicates better quality of the generated sentences (the experimental results in Figure 2 can verify this conclusion).
> 通常，困惑度值越小，语句的语言模型越好，这表明生成语句的质量越好（图2中的实验结果可以验证这一结论）。
📌 **分析：**
* 再次强调困惑度与文本质量的正向关系，并引用图2的实验结果作为佐证。

---

> So it can be used to represent perceptual-imperceptibility.
> 因此，它可以用作表示**感知不可察觉性（perceptual-imperceptibility）**。
📌 **分析：**
* 明确了困惑度作为衡量感知不可察觉性的指标。

---

> We have also calculated the per plexi t y of real sentences in the IMDB dataset and Twitter dataset, their mean value are 134.11 and 128.14, respectively.
> 我们还计算了 IMDB 数据集和 Twitter 数据集中真实语句的困惑度，它们的平均值分别为 134.11 和 128.14。
📌 **分析：**
* 提供了真实世界文本的困惑度均值作为参考基准。这有助于读者判断生成文本的困惑度是高是低，以及与真实文本的差距。

---

> Then, we can calculate the difference between the mean per plexi t y of the generated steganographic sentences and that of normal sentences under different embedding rates, which is expressed as M P: M P = |mean(P P L Stego) − mean(P P L Normal )| (28)
> 然后，我们可以计算在不同嵌入率下，生成的隐写语句的平均困惑度与正常语句的平均困惑度之间的差异，表示为 $\Delta MP$：
> $$\Delta MP = |\text{mean}(\text{PPL}_{\text{Stego}}) - \text{mean}(\text{PPL}_{\text{Normal}})| \quad \text{(28)}$$
📌 **分析：**
* **$\Delta MP$**：定义了一个新的指标，用于量化生成隐写文本在困惑度（质量）上与正常文本的差距。值越小，差距越小，感知不可察觉性越好。

---

> However, as analysised in Section III, per plexi t y cannot be used to measure the statistical-imperceptibility of generated steganographic sentences.
> 然而，正如第三节所分析的，困惑度不能用来衡量生成隐写语句的统计不可察觉性。
📌 **分析：**
* 再次强调了困惑度的局限性，引出需要使用其他指标来衡量统计不可察觉性。

---

> Then, we refered to Cachin’s analysis of the security of steganographic algorithm [34] and used the Kullback-Leibler divergence (KLD) between the overall distributions of generated steganographic sentences and normal sentences in feature space to evaluate statisticalimperceptibility.
> 然后，我们参考了卡钦（Cachin）对隐写算法安全性的分析 [cite: 34]，并使用生成隐写语句和正常语句在特征空间中整体分布之间的**库尔巴克-莱布勒散度（Kullback-Leibler divergence, KLD）**来评估统计不可察觉性。
📌 **分析：**
* **KLD 作为统计不可察觉性指标**：再次引用 Cachin 的信息论安全性理论，确立 KLD 作为衡量统计不可察觉性的主要指标。强调是在“特征空间”中计算 KLD，这意味着文本首先被映射到某种向量表示，然后计算这些向量分布的相似性。

---

> In addition, considering that KL divergence is not a strict distance measure (asymmetric), and recently some researchers have suggested using Jensen-Shannon divergence to measure the security of the generative steganography algorithm [54], [55], so we also calculated the Jensen-Shannon divergence (JSD) of the overall distributions between generated steganographic sentences and normal sentences, which is defined as follows: $D_{JS}(P_C||P_S ) = \frac{1}{2} D_{KL} (P_C|| \frac{P_C + P_S}{2}) + \frac{1}{2} D_{KL} (P_S || \frac{P_C + P_S}{2} )$. (29)
> 此外，考虑到 KL 散度不是一个严格的距离度量（非对称），并且最近一些研究人员建议使用詹森-香农散度来衡量生成式隐写算法的安全性 [cite: 54, 55]，所以我们还计算了生成隐写语句和正常语句之间整体分布的**詹森-香农散度（Jensen-Shannon divergence, JSD）**，其定义如下：
> $$D_{JS}(P_C||P_S ) = \frac{1}{2} D_{KL} (P_C||\frac{P_C + P_S}{2}) + \frac{1}{2} D_{KL} (P_S ||\frac{P_C + P_S}{2} ). \quad \text{(29)}$$
📌 **分析：**
* **JSD 作为补充指标**：引入 JSD 是因为 KLD 存在非对称性（$D_{KL}(P||Q) \ne D_{KL}(Q||P)$）。JSD 是一种对称的、且总是有界的度量，通常被认为在衡量概率分布相似性方面更为稳健和可靠。
* 公式 (29) 给出了 JSD 的数学定义，它基于两个 KL 散度的平均值。

---

> where, $P_C$ and $P_S$ represent the overall statistical distribution of normal text and generated steganographic text, respectively.
> 其中，$P_C$ 和 $P_S$ 分别代表正常文本和生成的隐写文本的整体统计分布。
📌 **分析：**
* 对公式 (29) 中符号的解释。

---

> In order to objectively reflect the performance of the proposed model, we also tested these indicators of the model proposed by Fang et al. [15] and Yang et al. [13].
> 为了客观地反映所提出模型的性能，我们还测试了 Fang 等人 [cite: 15] 和 Yang 等人 [cite: 13] 提出的模型的这些指标。
📌 **分析：**
* **对比模型**：明确了将与两个重要的基线模型（RNN-Stega 的两个版本）进行对比，以证明 VAE-Stega 的优越性。

---

> We trained each model on these two datasets, and for each embedding rates, we generated 1, 000 sentences for testing.
> 我们在两个数据集上训练了每个模型，并且对于每个嵌入率，我们生成了 1,000 条句子进行测试。
📌 **分析：**
* **实验规模和条件**：确保了对比实验的公平性和结果的统计可靠性（数据集、样本数量、不同嵌入率）。

---

> In order to ensure the objectivity of the evaluation, we used an independent third-party sentence mapping model [58] to embed steganographic sentences generated by different models with different embedding rates to the feature space and obtained their overall statistical distributions.
> 为了确保评估的客观性，我们使用了一个**独立的第三方语句映射模型** [cite: 58] 将不同模型在不同嵌入率下生成的隐写语句嵌入到特征空间，并获得了它们的整体统计分布。
📌 **分析：**
* **“独立的第三方语句映射模型”** [cite: 58]：这是确保 KLD 和 JSD 计算客观性的重要措施。如果使用隐写模型本身的编码器来映射，可能会存在偏差。使用一个独立的、与隐写过程无关的模型（例如 Doc2Vec 或 Skip-thought 等通用句子嵌入模型）将文本映射到特征空间，可以更公正地评估其统计分布相似性。

---

> Then we can calculate their KLD and JSD between the steganographic sentences and normal sentences on this feature space.
> 然后我们可以在这个特征空间上计算隐写语句和正常语句之间的 KLD 和 JSD。
📌 **分析：**
* 明确了 KLD 和 JSD 的计算是在独立的特征空间中进行的。

---

> The experimental results have been shown in Table I.
> 实验结果已在表I中显示。
📌 **分析：**
* 引导读者查看表I，其中包含了具体的数值结果。
![[Pasted image 20250521153437.png]]
---

> It is worth noting that according to our steganography strategy, the size of candidate pool will directly affect the final embedding rate.
> 值得注意的是，根据我们的隐写策略，**候选池的大小（size of candidate pool）**将直接影响最终的嵌入率。
📌 **分析：**
* **“候选池的大小”**：这是控制嵌入率的关键参数。候选池越大，能嵌入的比特数越多，反之则越少。

---

> But since we use variable length coding, there is no explicit correspondence between them.
> 但由于我们使用**变长编码（variable length coding）**，它们之间没有明确的对应关系。
📌 **分析：**
* **“变长编码”**：如霍夫曼编码或算术编码，其分配给每个词的比特数是可变的。这意味着即使候选池大小固定，实际的每词嵌入比特数（bpw）也会因生成文本内容的不同而有所浮动，因此需要进行实际计算。

---

> During the experiment, we adjusted the candidate pool size to be 2, 4, 8, 16, 32 ..., and then calculated the actual number of embedded bits and the length of the generated text.
> 在实验过程中，我们将候选池大小调整为 2, 4, 8, 16, 32 ...，然后计算了实际嵌入的比特数和生成文本的长度。
📌 **分析：**
* 描述了如何控制和测量不同嵌入率：通过调整候选池大小，并实际计算出对应的 bpw。

---

> Through the ratio of these two, we got the actual information embedding rate under different candidate pool sizes, that is, the bpw (bits per word) recorded in Table I (as well as Table II, Table III, Figure 2 and Figure 3).
> 通过这两者的比值，我们得到了不同候选池大小下的实际信息嵌入率，即表I（以及表II、表III、图2和图3）中记录的 **bpw（bits per word，每词比特数）**。
📌 **分析：**
* 明确了 bpw 的计算方式：`bpw = (总嵌入比特数) / (总生成词数)`。这是衡量嵌入效率的常用指标。
![[Pasted image 20250521153550.png]]
---

> Based on the results shown in Table I, we can draw the following conclusions.
> 根据表I中显示的结果，我们可以得出以下结论。
📌 **分析：**
* 引导读者对表I进行总结和分析，这部分是基于量化数据的结论。

---

> Firstly, as we analyzed in Section III, the optimization goal of the previous models, such as RNNStega [13], is to ensure that the conditional probability of each word in the generated steganographic sentences is as high as possible and thus to make the quality of the generated steganographic sentences is as good as possible.
> 首先，正如我们在第三节中分析的，以前的模型，如 RNN-Stega [cite: 13]，的优化目标是确保生成的隐写语句中每个词的条件概率尽可能高，从而使生成的隐写语句的质量尽可能好。
📌 **分析：**
* 重申了基线模型 RNN-Stega 的设计哲学：追求文本质量。

---

> It final results that the generated steganographic sentences have a low per plexi t y value.
> 最终结果是生成的隐写语句具有较低的困惑度值。
📌 **分析：**
* 印证了 RNN-Stega 在质量上的表现：困惑度低。

---

> However, the results lead to a huge difference between the overall statistical distribution of them and that of normal sentences, as shown in the calculation results of $\Delta MP$, KLD and JSD.
> 然而，这些结果导致它们与正常语句的**整体统计分布（overall statistical distribution）**之间存在巨大差异，如 $\Delta MP$、KLD 和 JSD 的计算结果所示。
📌 **分析：**
* **核心对比**：再次强调了 RNN-Stega 的缺点——虽然质量好，但整体统计分布与正常文本差异大，从而导致安全风险。这是对 Psic Effect 的实验验证和量化。

---

> Secondly, we found that although the previous experimental results of Ziegler et al. [17] showed that in the process of text generation, the use of arithmetic coding on the conditional probability distribution can reduce the difference between the generated steganographic text and normal text in the words’ conditional probability distribution compared with Huffman coding.
> 其次，我们发现，尽管 Ziegler 等人 [cite: 17] 先前的实验结果表明，在文本生成过程中，与霍夫曼编码相比，在条件概率分布上使用**算术编码（arithmetic coding）**可以减少生成隐写文本与正常文本在词语条件概率分布上的差异。
📌 **分析：**
* 引入了算术编码的理论优势：在局部（条件概率分布）层面比霍夫曼编码更能保持统计相似性。

---

> However, by comparing the KLD and JSD of RNN-Stega (HC) and RNN-Stega (AC), due to they (both Yang et al. [13] and Ziegler et al. [17]) lack constraints on the overall statistical distribution of the text, the steganographic text generated by these two coding methods still have a big difference in the overall statistical distribution with that of normal text (arithmetic coding is slightly better than Huffman coding).
> 然而，通过比较 RNN-Stega (HC) 和 RNN-Stega (AC) 的 KLD 和 JSD，由于它们（包括 Yang 等人 [cite: 13] 和 Ziegler 等人 [cite: 17] 的工作）缺乏对文本整体统计分布的约束，这两种编码方法生成的隐写文本在整体统计分布上与正常文本仍然存在很大差异（算术编码略优于霍夫曼编码）。
📌 **分析：**
* **关键洞察**：作者指出，即使像算术编码这样在局部统计上更优的方法，在缺乏对“整体统计分布”约束的情况下，仍然无法有效解决隐写安全性问题。这进一步强调了本文 VAE-Stega 模型的创新点——对**整体分布**的关注。

---

> Thirdly, we found that after we introducing the VAE architecture, the model can therefore learn the overall statistical distribution characteristics of normal text and further constrain that of the generated steganographic text to a certain degree, which can significantly reduce the difference between the overall statistical distribution of generated steganographic sentences and that of normal sentences, i.e. greatly improve the statistical-imperceptibility, in the case of a little loss of per plexi t y (not much, and it will not significantly affect the perceptual-imperceptibility of the generated steganographic sentences, which can be found in the Supplemental Materials).
> 第三，我们发现，在我们引入 VAE 架构后，模型因此可以学习正常文本的**整体统计分布特征（overall statistical distribution characteristics）**，并在一定程度上进一步约束生成隐写文本的分布，这可以显著减少生成隐写语句与正常语句之间整体统计分布的差异，即大大提高**统计不可察觉性（statistical-imperceptibility）**，而困惑度损失很小（不多，并且不会显著影响生成隐写语句的感知不可察觉性，这可以在补充材料中找到）。
📌 **分析：**
* **VAE-Stega 的核心优势总结**：
    * **学习整体分布**：VAE 架构使模型能够捕捉正常文本的宏观统计特性。
    * **显著提高统计不可察觉性**：通过对整体分布的约束，KLD 和 JSD 显著降低。
    * **平衡效果**：在大幅提高统计不可察觉性的同时，对感知不可察觉性（困惑度）的损失很小，这意味着文本质量仍然保持在可接受的范围内。
* 明确了 VAE-Stega 成功解决了 Psic Effect，实现了感知和统计不可察觉性之间的平衡。
* **“补充材料”**：表明论文有额外的附录来支持其关于感知不可察觉性损失很小的结论。