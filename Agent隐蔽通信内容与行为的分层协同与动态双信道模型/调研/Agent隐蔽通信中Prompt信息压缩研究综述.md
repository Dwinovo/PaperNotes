## 一、 引言：Agent隐蔽通信的挑战与Prompt的角色

"Agent隐蔽通信内容与⾏为的分层协同与动态双信道模型"。其核心思想是构建两个并行的通信信道：
- 一个高带宽的"内容信道"，负责传输主要的、大容量的秘密信息；
- 一个低带宽但关键的"行为信道"，利用Agent在环境中的交互行为模式（如消息发布频率、时间间隔、对特定内容的互动等）来传递控制和同步信号。
这种分层协同机制，特别是通过行为信道传递Prompt变更等元信息，旨在解决内容信道的静态Prompt同步瓶颈。

在此双信道模型中，行为信道的带宽通常极为有限。因此，若要通过行为信道隐蔽、可靠地传输Prompt本身或其变更指令（例如，切换到新的Prompt模板，或对现有Prompt进行微调的参数），对Prompt信息进行高效的压缩与精确的解压缩便成为至关重要的核心技术前提。未经压缩的Prompt信息量巨大，远非低带宽的行为信道所能承载。因此，研究适用于此类场景的Prompt压缩与解压缩方法，对于实现灵活、鲁棒且自主的Agent隐蔽通信具有决定性意义。

Prompt 压缩需要关注**三大维度**：
1. **尺寸与重量**：适应行为信道极低带宽的轻量化设计，将复杂 Prompt 转化为少量比特流 。
2. **装药与引信**：精确编码与可靠解码，确保低带宽信号能准确承载 Prompt 变更指令，并被接收方Agent无误解码 。
3. **威力**：恢复后的 Prompt 能准确指导 Agent 执行任务，关注保真度和行为一致性 。
## 二、 主流Prompt压缩技术分类、原理与评估

### 1. 索引化与查表方法 (Indexing & Look-up Table)

**基本原理**：预先在通信双方共享一个静态或可扩展的Prompt模板库，每个模板赋予唯一标识符（如数字ID或哈希值）。通信时，仅传递所需Prompt的标识符，接收方查表获取完整内容。

**关键技术细节**：
- **静态查表法**：最简单的形式，预设固定Prompt集合，双方在通信前就达成共识。例如，《行为模式-弹道选择》中提到的"基于共享内存的交互模式选择"，可预先分配Prompt表，通过特定行为模式（如，选择不同交互对象的顺序）来实际传递Prompt ID。
- **动态字典构建**：借鉴LZ系列压缩算法思想，随着交互进行逐步构建共享字典。通过在内容信道传输的文本内容中隐含地增加新Prompt模板，然后通过行为信道传递使用指示。
- **层级索引系统**：对Prompt按功能类别、应用场景等进行层级分类，形成树状索引结构，减少传输所需比特数，且便于系统化管理和扩展。
- **哈希映射优化**：利用哈希函数将Prompt映射至紧凑标识符，配合避免碰撞的设计，可支持极大的可寻址空间。

**优势**：
- 极高的压缩率，仅需传递极少比特即可表示复杂Prompt
- 零解压缩误差，完美还原预设Prompt
- 解码速度极快，几乎无计算延迟

**局限性**：
- 缺乏灵活性，难以传递全新或微调的Prompt
- 预设库规模与更新是瓶颈，大库难以隐蔽同步
- 可能通过统计分析发现异常选择模式

**适用场景**：适合预定义任务集有限、变化性低的隐蔽通信，特别是对行为信道带宽极为受限的情况。若配合参数化模板使用（见下节），可大幅提升灵活性。

### 2. 参数化模板与差分编码 (Parameterized Templates & Differential Encoding)

**基本原理**：参数化模板将Prompt结构化为基础模板+可变参数，仅传递变化的参数值大幅减少信息量；差分编码则只传递相对上一版本Prompt的变化部分。

**关键技术细节**：
- **参数化Prompt模板**：设计包含变量槽位的模板（如`{role}`执行`{action}`的任务，其中`{constraint}`），仅传递变量值（role="研究者", action="分析数据", constraint="24小时内"）。变量槽可预定编号，极大压缩信息。
- **插槽设计优化**：针对特定领域，研究最优的变量槽设计和粒度，平衡表达能力与压缩率。例如，《双信道模型》中提到的"精确语法"与"平行语义"变量粒度选择策略。
- **差分编码技术**：类似Git版本控制，记录基准Prompt与目标Prompt的差异。支持多种差分计算方法：
  - 文本级差分：记录添加/删除/修改的文本段
  - 标记级差分：更细粒度地跟踪Token变化
  - 语义差分：基于嵌入空间的语义变化描述
- **变更指令压缩**：将差分操作本身编码为极简指令（如"在第3行'执行'后插入'迅速'一词"）

**优势**：
- 显著的压缩率提升，尤其对结构化场景
- 保持高度灵活性，支持新Prompt构建
- 易于适配变化频繁的动态环境

**局限性**：
- 需预共享模板库或基准版本
- 参数化设计对语言和表达的限制
- 差分编码对基准版本的依赖性高

**适用场景**：适合结构化程度高、变化相对有限的Prompt传递。在基于"模式-弹道选择"的隐蔽通信中，可优化配合时间型或交互型行为信道，使用有限比特传输关键参数变更。

### 3. 基于模型的Prompt信息压缩方法

**基本原理**：将Prompt转换为语义向量空间中的表示，利用降维技术压缩后传输，接收方重构向量并生成等效Prompt。这种方法依托于大型语言模型内在的语义理解能力。
#### 1. 500xCompressor: Generalized Prompt Compression for Large Language Models (论文1)
* **核心特点**：利用LLM自身进行端到端压缩，将自然语言上下文压缩至极少数（最少一个）特殊token [cite: 3]。
* **信息表示**：关键信息主要存储在这些特殊token的**KV值 (Key-Value values from attention layers)**中，而非仅仅是它们的嵌入 [cite: 8]。
* **模型修改**：编码器LLM使用冻结参数并配备可训练的LoRA参数进行训练 [cite: 12, 44]。解码器则使用原始冻结的LLM，无需额外微调 [cite: 12, 44]。
* **训练方式**：包含预训练（文本再生任务）和指令微调（问答任务）两个阶段 [cite: 13, 15]。
* **优势**：实现了极高的压缩率（6x-480x） [cite: 4]，压缩后的提示可直接被原始LLM使用 [cite: 5]，对复杂信息（如专有名词、数字）有较好的压缩和检索能力 [cite: 27]。

#### 2. AN EMPIRICAL STUDY ON PROMPT COMPRESSION FOR LARGE LANGUAGE MODELS (论文2)
* **核心特点**：这是一篇**实证研究**，本身不提出新的压缩方法，而是对现有的六种提示压缩方法（KiS, SCRL, Selective Context, LLMLingua, LongLLMLingua, LLMLingua-2）进行多维度综合评估 [cite: 3, 43, 55, 88]。
* **评估维度**：涵盖生成性能、模型幻觉、多模态任务中的效用、词语省略分析、响应长度等 [cite: 4, 30, 31]。
* **关键发现**：(Long)LLMLingua 和 LLMLingua-2总体表现较好，尤其在高压缩比下 [cite: 26, 54]。长上下文中适度压缩可能提升性能 [cite: 6, 7, 27]。所有方法都可能增加幻觉，信息丢失是主因 [cite: 29, 156]。
* **贡献**：提供了全面的比较基准和统一的开源工具包(PCToolkit) [cite: 7, 31]。

#### 3. Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt (论文3)
* **核心特点**：该论文的重点**并非压缩输入给LLM的Prompt**，而是通过学习一种**可迁移的软提示 (soft prompt)** 来提升**已经被模型压缩技术（如量化、剪枝）处理过的LLM**的性能 [cite: 111, 112]。
* **"Prompt"的角色**：这里的“提示”是一系列可学习的嵌入向量，附加在输入序列前，用于“纠正”或“补偿”因模型压缩带来的性能损失 [cite: 112, 116]。
* **压缩感知**：软提示的学习过程考虑了LLM的压缩状态，使其能适应并纠正压缩偏差 [cite: 114, 116]。
* **可迁移性**：学习到的软提示可以在不同数据集、任务和压缩级别/类型之间迁移 [cite: 112, 114]。
* **优势**：显著提升极端压缩LLM的性能 [cite: 112, 119]，且对推理延迟影响小 [cite: 115, 124]，无需微调LLM主体参数 [cite: 115]。

#### 4. Covert Prompt Transmission for Secure Large Language Model Services (论文4)
* **核心特点**：提出一个两阶段安全LLM服务方案，重点是隐蔽的提示传输，包含提示压缩与加密 (PCAE) 框架和基于深度强化学习 (GPPO) 的隐蔽无线传输优化 [cite: 126, 128, 129]。
* **PCAE压缩方法**：
    * 利用**本地部署的小型语言模型 (SLM)** 估计词元级别的**惊奇度 (surprisal) 分数** [cite: 3, 4, 129, 132]。
    * 选择性保留语义关键（高惊奇度）的词元，丢弃冗余词元 [cite: 4, 129, 133]。
    * 结合轻量级的基于置换的加密 [cite: 3, 129, 145]。
* **GPPO优化**：基于分组的近端策略优化 (GPPO)，用于联合优化提示压缩率和发射功率，以最小化传输延迟，同时满足保真度和隐蔽性约束 [cite: 5, 45, 129, 133]。
* **优势**：PCAE预处理延迟极低，适合边缘部署 [cite: 6, 49, 129]。GPPO能有效降低隐蔽传输延迟 [cite: 8, 50, 129]。

#### 5. Dynamic Compressing Prompts for Efficient Inference of Large Language Models (LLM-DCP) (论文5)
* **核心特点**：将提示压缩建模为**马尔可夫决策过程 (MDP)** [cite: 6, 148, 151]，由一个DCP-Agent（基于Transformer编码器）通过强化学习（PPO算法）顺序移除冗余词元 [cite: 6, 148, 151, 157]。
* **奖励函数设计**：平衡压缩率、输出质量（通过与一个经过“分布对齐”的小型语言模型比较KL散度）和关键信息保留（BERTScore） [cite: 7, 48, 153, 160]，**不依赖目标黑盒LLM进行训练监督** [cite: 8, 49, 153]。
* **训练策略**：引入**分层提示压缩 (HPC) 训练策略**，借鉴课程学习思想，逐步增加压缩难度 [cite: 9, 50, 153, 165]。
* **优势**：任务无关的动态压缩 [cite: 4, 151]，训练成本较低 [cite: 8, 49, 154]，对动态上下文适应性强 [cite: 6, 151, 250]，在高压缩率下性能优越 [cite: 10, 153, 167]。

#### 6. ICPC: In-context Prompt Compression with Faster Inference (论文6)
* **核心特点**：利用**预训练的Transformer编码器（非完整LLM）** 计算词元概率和携带的信息量（损失）来进行压缩 [cite: 14, 15, 180, 183]，旨在实现极快的压缩速度 [cite: 15, 180]。
* **信息量计算**：对每个“分词单元”（词、短语或子句） [cite: 20, 38, 184]，其移除损失综合考虑了对上下文连贯性的影响和该单元自身的条件概率 [cite: 41, 187, 191]。
* **过滤机制**：基于所有单元损失值的**p-th百分位数**设定动态阈值，移除损失值大于等于该阈值的单元 [cite: 43, 44, 184, 187]。
* **优势**：压缩过程本身速度极快（比依赖LLM的方法快10-100倍） [cite: 18, 183, 191]，计算和内存开销低 [cite: 3, 183, 192]，同时保持了较好的压缩性能和通用性（在多种编码器上有效） [cite: 6, 15, 183, 185]。

#### 7. PromptOptMe: Error-Aware Prompt Compression for LLM-based MT Evaluation Metrics (论文7)
* **核心特点**：针对基于LLM的**机器翻译 (MT) 评估指标 (以GEMBA-MQM为起点)** [cite: 5, 212]，提出一种**错误感知 (error-aware)** 的提示压缩优化方法 [cite: 3, 212, 219]。
* **压缩模型**：使用一个更小的、经过微调的语言模型（如LLaMA-3.2）进行压缩 [cite: 3, 214, 219]。
* **两阶段微调**：
    * **监督微调 (SFT)**：训练模型学习压缩任务，并识别和保留源文本与MT文本中的潜在**错误跨度** (利用MQM标注数据) [cite: 4, 12, 67, 214, 219]。
    * **偏好优化 (使用ORPO算法)**：根据实际LLM评估指标（如GPT-40驱动的GEMBA-MQM）对不同压缩版本给出的分数差异，构建偏好数据，进一步细化压缩模型，使其选择能最好保持评估质量的压缩版本 [cite: 4, 13, 68, 214, 219]。
* **优势**：在不损失MT评估质量的情况下显著减少令牌使用量（高达2.37倍） [cite: 6, 34, 214]，使昂贵的LLM评估指标更具成本效益 [cite: 7, 214]。其针对性优化使其优于通用压缩方法 [cite: 162, 182, 183, 214]。

#### 8. PIS: Linking Importance Sampling and Attention Mechanisms for Efficient Prompt Compression (论文8)
* **核心特点**：提出一种新颖的**提示重要性采样 (PIS)** 框架 [cite: 4, 228]，通过分析LLM原生的**注意力机制**的输出来采样重要token [cite: 4, 228]，从而动态压缩提示，并以**测度论**提供理论基础 [cite: 21, 29, 228, 234]。
* **双层压缩机制**：
    * **Token层面 (TIS)**：使用LLM（或小型编码器代理）的注意力得分（经TF-IDF校正）量化token显著性 [cite: 5, 106, 228, 234]。通过一个轻量级9层强化学习网络 (DDQN) 实现句子粒度的自适应压缩比率选择 [cite: 5, 112, 228, 234]。
    * **语义/句子层面 (SIS)**：在TIS处理后，基于句子间语义相似度（余弦相似度） [cite: 124, 228]，采用**俄罗斯轮盘赌采样策略**概率性移除冗余句子 [cite: 6, 123, 228, 234]。
* **优势**：无需外部生成模型 [cite: 22, 27, 235]，计算成本相对较低 [cite: 235]。在压缩质量和效率上均表现优越 [cite: 7, 25, 234]，有时优化的上下文结构甚至能提升下游任务性能 [cite: 8, 28, 234, 244]。

#### 9. Leveraging Attention to Effectively Compress Prompts for Long-Context LLMs (AttnComp) (论文9)
* **核心特点**：利用语言模型内部的**注意力机制**来指导提示压缩，解决传统信息熵指标的不足和token独立性假设问题 [cite: 8, 249, 252]。
* **重要性度量**：使用从**查询 (query) 到上下文 (context) 的因果交叉注意力 (CA)** (特别是从特定“检索头”提取) 作为评估token重要性的新指标 [cite: 9, 28, 29, 252, 253]。
* **语义单元识别**：开发了一种基于图的算法，利用上下文内部的**自注意力 (SA)** 构建图 [cite: 9, 34, 252]，并通过**最大生成树 (MST) 和社区检测 (Louvain算法)** 将token聚类成语义单元 [cite: 9, 34, 48, 49, 252]，在单元级别进行压缩决策 [cite: 252]。
* **优势**：提出的交叉注意力指标比PPL更优 [cite: 171, 172, 253, 259]。通过语义单元克服了独立性假设，更好地保留语义完整性 [cite: 35, 253, 261]。在性能和延迟方面均优于先前基线 [cite: 11, 12, 253, 260]。



**关键技术细节**：
- **嵌入空间映射**：利用预训练模型的编码器，将整个Prompt或其结构化部分映射到高维嵌入向量。
- **低维投影技术**：应用主成分分析(PCA)、自动编码器(AE)、变分自动编码器(VAE)等技术将高维向量压缩到极低维度（如32-128维），保留主要语义信息。
- **Prompt重构方法**：
  - **向量解码重构**：使用专用解码器将压缩向量重构为自然语言Prompt
  - **语义引导生成**：使用压缩向量作为语义控制信号，引导LLM生成功能等效的新Prompt
  - **语义检索匹配**：在接收端维护语义索引库，搜索语义最接近的预存Prompt
- **量化与精度控制**：应用向量量化技术（如Product Quantization），进一步压缩传输比特数
可以向NLP领域中的
**优势**：
- 极高的压缩率，特别适合长文本Prompt
- 语义级保真度，关注功能等效而非完全复制
- 支持跨语言、跨模型的Prompt转换

**局限性**：
- 有损压缩，存在语义偏移风险
- 重构质量依赖于模型能力
- 计算开销较大，不适用资源受限场景
- 需预共享编解码器模型或参数

**适用场景**：适合要求功能等效但允许表述灵活的Prompt传递，特别是在行为信道带宽严格受限但通信双方计算资源较丰富的情况。在基于内容与行为双信道的隐蔽通信中，可用于传递全新任务指令或复杂控制逻辑。


